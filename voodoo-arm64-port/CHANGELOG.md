# Voodoo ARM64 JIT Port -- Changelog

All changes, decisions, and progress for the ARM64 port of the Voodoo GPU pixel pipeline JIT.

---

## Batch 10: Prologue pointer load optimizations (2026-02-21)

Two optimizations from the Round 2 audit: R2-23, R2-09.
Saves ~13 instructions per JIT block (prologue-only, amortized over span).

### R2-23: Skip zero halfwords in EMIT_MOV_IMM64 (~10 insns/block)
The `EMIT_MOV_IMM64` macro loaded 64-bit pointers using a fixed 4-instruction sequence
(MOVZ + 3 MOVK) regardless of the pointer value. On macOS ARM64, user-space pointers are
in the range `0x0000_0001_xxxx_xxxx`, so the top halfword (hw=3) is always zero. Additionally,
the bottom halfword is often zero for page-aligned data. The optimized version finds the first
non-zero halfword and uses `MOVZ Xd, #imm16, LSL #(hw*16)` (which zeros all other bits), then
only emits MOVK for remaining non-zero halfwords. Applied to 4 sites: EMIT_MOV_IMM64 (7 prologue
pointer loads), EMIT_LOAD_NEON_CONST (3 NEON constant loads), and dither_rb_addr (1 dither
table pointer). New `ARM64_MOVZ_X_HW(d, imm16, hw)` encoding macro added.

### R2-09: MOVZ with hw=3 for (1<<48) constant (3 insns/block)
The perspective texture division dividend `0x0001_0000_0000_0000` (1<<48) was loaded with 4
instructions: `MOVZ X4, #0` + 3 MOVK (inserting zeros for hw=1,2 and finally #1 for hw=3).
Since only bit 48 is set, a single `MOVZ X4, #1, LSL #48` (hw=3) suffices -- it places 1 in
bits [63:48] and zeros all other bits. Saves 3 instructions per perspective-textured block.

---

## Batch 9: Texture + color broadcast optimizations (2026-02-21)

Three optimizations from the Round 2 audit: R2-07, R2-12, R2-08.
Saves ~3-5 instructions/pixel (path-dependent).

### R2-07: Eliminate ebp_store memory round-trip (2 insns/pixel, bilinear path)
The bilinear texture section stored the bilinear lookup index to `STATE_ebp_store` (memory),
then reloaded it ~40 instructions later. The value was kept in w10 which got clobbered
in the intervening texture coordinate setup. Fix: hold the value in w17 (IP1 scratch register)
instead of round-tripping through memory. Since there are no BL/BLR calls in the generated
pixel loop, w17 is safe to use as a long-lived scratch. Eliminates 1 STR + 1 LDR, adds 1 MOV
= net 1 instruction saved per bilinear-textured pixel.

### R2-12: DUP_V4H_GPR replaces FMOV+DUP (1 insn per site, 8 sites)
The scalar-to-vector broadcast pattern `FMOV Sd, Wn` + `DUP Vd.4H, Vd.H[0]` (2 instructions)
is replaced by `DUP Vd.4H, Wn` (1 instruction). The `DUP (general)` variant takes the low
16 bits of a GPR and broadcasts to all halfword lanes. All 8 affected sites broadcast values
that are provably 0-255 (alpha values, LOD fractions, detail blend factors), so the 16-bit
truncation is safe. Sites: TC_MSELECT_DETAIL (TMU0+TMU1), TC_MSELECT_LOD_FRAC (TMU0+TMU1),
CC_MSELECT_AOTHER, CC_MSELECT_ALOCAL (both paths), CC_MSELECT_TEX.

### R2-08: Cache original LOD for point-sample clamp/wrap (1 insn/pixel, point-sample path)
In the point-sample texture path, `ADD w6, w6, #4` modifies the cached LOD (w6) to create
the point-sample shift amount. The subsequent S/T clamp/wrap sections need the *original*
LOD for indexing into tex_w_mask/tex_h_mask arrays, so they reloaded it from `STATE_lod`
memory. Fix: insert `MOV w11, w6` before the ADD to save the original LOD in w11, which
is free at that point. Eliminates 2 LDR instructions (one for S, one for T... wait, only
the S section loaded it; the T section reused w11 from S). Net: 1 instruction saved per
point-sampled pixel.

---

## JIT cache expansion: 8 -> 32 slots per thread (2026-02-21)

Increased `BLOCK_NUM` from 8 to 32, giving each of the 4 render threads 32 cache slots
(128 total, up from 32). Real workloads hit 180-351 unique pipeline configurations;
the old 8-slot cache caused massive recompilation churn (258K recompilations for 351
configs in a single Q3 session). With 32 slots, the working set fits comfortably and
most recompilations are eliminated after warmup.

### Memory budget
- Old: 32 slots x 16KB = 512KB code memory
- New: 128 slots x 16KB = 2MB code memory (well under 4MB budget)
- Metadata: 128 x sizeof(voodoo_arm64_data_t) ~ 10KB (negligible)

### Changes
- `BLOCK_NUM`: 8 -> 32, `BLOCK_MASK`: 7 -> 31
- Cache search loop: `c < 8` -> `c < BLOCK_NUM`
- All round-robin eviction masks: hardcoded `& 7` -> `& BLOCK_MASK`
- Comments updated to reflect 32-entry ring buffer

### Platform compatibility
- macOS: `pthread_jit_write_protect_np` toggles per-thread (not per-page), unaffected
- Linux/Pi 5: `mprotect` per-block unchanged (each slot has its own mmap region)
- Windows ARM64: `VirtualProtect` per-block unchanged

---

## Batch 8: Loop + CC + Stipple peepholes (2026-02-21)

Re-applied 4 safe, mechanical optimizations from the Round 2 audit (R2-24, R2-25, R2-13, R2-27).
Saves ~4 instructions/pixel total.

### R2-24: Move STATE_x LDR before loop (1 insn/pixel)
The `LDR w28, [x0, #STATE_x]` was inside the loop body, reloading every iteration. Since
`MOV w28, w5` at the loop tail already keeps w28 current, the load only needs to run once
before the first iteration. Moved it above `loop_jump_pos`.

### R2-25: Eliminate MOV w4,w28 in loop control (1 insn/pixel)
The loop tail had `MOV w4, w28` then `ADD/SUB w5, w4, #1`. Changed to `ADD/SUB w5, w28, #1`
directly. Reordered CMP before `MOV w28, w5` so CMP reads the old x value.

### R2-13: Eliminate redundant MOV v16,v0 in cc multiply (1 insn/pixel)
`MOV v16, v0` saved v0 before the blend factor EOR/ADD, then SMULL read v16. Since only v3
(blend factor) is modified between the MOV and SMULL, v0 is untouched. Removed the MOV and
changed SMULL to read v0 directly: `SMULL v17.4S, v0.4H, v3.4H`.

### R2-27: MVN directly from w28 in stipple (1 insn/pixel, stipple path)
Pattern stipple had `MOV w5, w28` + `MVN w5, w5`. Since ARM64 MVN supports different src/dst
registers, replaced with single `MVN w5, w28`.

---

## Bugfix: Batch 7/M5 TMU0 Alpha Extraction Ordering (2026-02-21)

### Root cause

Batch 7/M5 refactored the dual-TMU TCA (texture combine alpha) section to extract the
TMU0 alpha byte from v7 (packed BGRA) into w13 once, then reuse w13 at multiple consumer
sites via `MOV w5, w13`. However, the extraction (`FMOV w13, s7` + `LSR w13, w13, #24`)
was placed at the START of the TCA section -- AFTER code that already reads w13.

Specifically, when `tca_sub_clocal` is true, the code emitted `MOV w5, w13` (to save the
TMU0 alpha for the subtract step) immediately after the TC multiply, but w13 had not been
extracted yet. This caused w13 to contain a stale value from earlier in the pipeline,
producing incorrect alpha values in the TCA subtract path.

### The fix

Moved the w13 extraction to BEFORE the SMULL (TC multiply), which is the earliest point
after v7 is established and before any code reads w13. The old extraction site at the TCA
section start was replaced with a comment noting w13 is already populated.

### Verification

- The FMOV_W_S(13, 7) reads s7 (scalar lane 0 of v7) and writes w13 -- no conflict with
  v4 which is the SMULL blend factor
- The SMULL/SSHR/SQXTN sequence operates on v16/v1/v4 -- none touch w13 (GPR)
- All downstream uses of w13 (tca_sub_clocal, TCA_MSELECT_CLOCAL, TCA_MSELECT_ALOCAL,
  tca_add_clocal) now see the correct extracted alpha value

### LDP pairing audit (Batch 7/M1) -- all verified correct

Checked all 4 LDP pairing sites against struct layout:
- `LDP w4, w5, [x0, #STATE_tex_s]`: tex_s=188, tex_t=192, diff=4 (adjacent int32)
- `LDP x5, x6, [x0, #STATE_tmu_s(tmu)]`: tmu0_s=496, tmu0_t=504, diff=8 (adjacent int64)
- `LDP x8, x9, [x0, #STATE_fb_mem]`: fb_mem=456, aux_mem=464, diff=8 (adjacent pointers)
- All confirmed adjacent in `voodoo_state_t` with compile-time VOODOO_ASSERT_OFFSET checks

#### File changed:
- `src/include/86box/vid_voodoo_codegen_arm64.h`

---

## Optimization Batch 7: Misc Small Wins + Dead Code Removal (2026-02-20)

### M1: LDP for adjacent 32-bit/64-bit loads

Combined adjacent individual loads into paired load instructions at 4 sites:
- Bilinear tex_s + tex_t: `LDP w4, w5` replacing two `LDR w` (STATE_tex_s, STATE_tex_t)
- Point-sample tex_s + tex_t: same LDP pairing
- Prologue fb_mem + aux_mem: `LDP x8, x9` replacing two `LDR x` (STATE_fb_mem, STATE_aux_mem)
- Perspective tmu_s + tmu_t: `LDP x5, x6` replacing two `LDR x` (TMU0 only; TMU1 offsets
  exceed LDP signed-7 range, guarded by runtime check)

Saves 4 instructions per compiled block.

### M3: Hoist fogColor to v11 (replacing dead neon_minus_254)

fogColor (`params->fogColor`) is triangle-invariant but was loaded and unpacked per-pixel
in the fog section (LDR + FMOV + UXTL). Now loaded once in the prologue into callee-saved
v11 when fog is enabled.

- FOG_CONSTANT path: `UQADD v0, v0, v11` directly (was 3 insns: LDR + FMOV + UQADD)
- Non-constant fog path: `UXTL v3, v11` widens to 16-bit (was 3 insns: LDR + FMOV + UXTL)

Saves 2-3 instructions per fog-enabled pixel.

### M5: Extract TMU0 alpha once in dual-TMU TCA

In the dual-TMU texture combine alpha (TCA) section, the pattern `FMOV w, s7` +
`LSR w, w, #24` (extract TMU0 alpha byte from packed BGRA) appeared at up to 4 sites.
Now extracted once into w13 and reused via `MOV w5, w13` at each consumer.

**BUG**: Original placement was at TCA section start, AFTER code that reads w13 via
`tca_sub_clocal`. Fixed 2026-02-21: extraction moved to before the SMULL.

Saves 2-6 instructions depending on TCA configuration.

### M6: BFI for no-dither RGB565 packing

Replaced the 7-instruction no-dither RGB565 packing sequence with a 6-instruction version
using BFI (Bit Field Insert). New `ARM64_BFI` encoding macro added.

Old: UBFX blue + UBFX green + LSL green + UBFX red + LSL red + ORR R|G + ORR |B (7 insns)
New: UBFX blue + UBFX green + UBFX red + LSL red + BFI green + ORR blue (6 insns)

Saves 1 instruction per non-dithered pixel.

### L1: CBZ for tmu_w divide-by-zero guard

Replaced `CMP x7, #0` + `B.EQ` with `CBZ x7` at the perspective W division guard.
Added `ARM64_CBZ_X_PLACEHOLDER` and `ARM64_CBNZ_X_PLACEHOLDER` encoding macros.

Saves 1 instruction per perspective-corrected texture fetch.

### L2: Eliminate MOV w11,w7 before LSL in texture fetch

In both bilinear and point-sample texture fetch paths, `MOV w11, w7` copied tex_shift
before `LSL w5, w5, w11`. Since w7 is not modified between the copy and use, the LSL
now operates on w7 directly: `LSL w5, w5, w7`.

Saves 2 instructions (1 per texture fetch variant).

### Bonus: Remove dead neon_minus_254

Removed the dead `neon_minus_254` constant and its prologue load sequence:
- Removed C-level static initialization (4 lines)
- Removed `EMIT_LOAD_NEON_CONST(11, &neon_minus_254)` which expanded to 5 ARM64 instructions

v11 is now repurposed for fogColor (M3 above). When fog is disabled, v11 is simply
unused (still saved/restored as part of the callee-saved d10/d11 pair).

Saves 5 prologue instructions.

### Feature parity audit

Full 17-stage feature parity audit confirmed ARM64 codegen matches x86-64 reference
across all pipeline stages after all 7 optimization batches. No features removed or
degraded. Report: `voodoo-arm64-port/feature-parity-audit.md`.

#### File changed:
- `src/include/86box/vid_voodoo_codegen_arm64.h`

---

## Optimization Batch 6: Cache LOD + Iterated BGRA (2026-02-20)

### H4: Cache LOD in w6 after store to STATE_lod

After computing LOD and storing it to `STATE_lod`, the value is now kept in w6
via a `MOV` instruction. This eliminates 3 redundant `LDR w6, [x0, #STATE_lod]`
memory reloads:
- Bilinear texture path: 2 reloads eliminated (first load + "reload for array indexing")
- Point-sample texture path: 1 reload eliminated

Both the perspective and non-perspective LOD computation paths now `MOV` their
result into w6 before falling through to the texture lookup code.

Saves 1 instruction per pixel (-3 LDR + 2 MOV), but more importantly removes
3 memory loads from the critical texture fetch path.

### H6: Cache iterated BGRA in v6 (NEON scratch)

The packed iterated BGRA computation (5 instructions: ADD\_IMM\_X + LD1 + SSHR +
SQXTN + SQXTUN) appeared at 4 sites in the color combine section, all computing
the identical value from `ib/ig/ir/ia`. These values don't change during color
combine (only updated at end of pixel loop).

Added a one-time computation into NEON register v6 before color combine starts,
guarded by a C-level `needs_iter_bgra` condition. Each of the 4 original sites
is replaced with a single instruction:

- Chroma key ITER\_RGB: 6 instructions → `FMOV w4, s6` (1 instruction)
- Color local select (iter path): 5 instructions → `MOV v1, v6` (1 instruction)
- Color local select override (iter branch): 5 instructions → `MOV v1, v6` (1 instruction)
- Color other select ITER\_RGB: 5 instructions → `MOV v0, v6` (1 instruction)

**Note**: v14 was originally planned for this cache but is already used for TMU1
ST deltas (Batch 3). v6 (NEON) is free during the entire color combine section
and is a different physical register from w6 (GPR, used for LOD in H4).

#### File changed:
- `src/include/86box/vid_voodoo_codegen_arm64.h`

---

## Optimization Batch 5: Pin rgb565 Pointer + Pair Counters (2026-02-20)

### M4: Pin rgb565 lookup table pointer in x26

Loaded the `rgb565` decode table pointer into callee-saved x26 in the prologue
via `EMIT_MOV_IMM64(26, &rgb565)`. x26 was already saved/restored as part of
the x25/x26 STP/LDP pair.

In the alpha blend path, replaced the per-pixel 4-instruction MOVZ+MOVK sequence
with a direct indexed load from x26: `LDR w6, [x26, w6, UXTW #2]`. Saves 3
instructions per alpha-blended pixel.

### M7: Pair pixel_count + texel_count with LDP/STP

Added `ARM64_LDP_OFF_W` and `ARM64_STP_OFF_W` encoding macros for 32-bit
load/store pair with signed offset.

When textures are enabled (common case), pixel_count and texel_count are now
loaded and stored as a pair. Since STATE_pixel_count (552) exceeds the LDP
W-form imm7 range (max 252), an `ADD x7, x0, #STATE_pixel_count` computes
the base address first, then LDP/STP at offset 0.

Saves 1 instruction per pixel when textures are enabled.

#### File changed:
- `src/include/86box/vid_voodoo_codegen_arm64.h`

---

## Optimization Batch 4: BIC+ASR Clamp Idiom (2026-02-20)

### M2: Replace 5-instruction clamp with 3-instruction ARM idiom

Replaced all 9 clamp-to-[0,255] sites from the old 5-instruction pattern
(`CMP #0` / `CSEL LT` / `MOVZ #0xFF` / `CMP #0xFF` / `CSEL GT`) with the
3-instruction ARM idiom:
```
MOVZ w10, #0xFF
BIC  wN, wN, wN, ASR #31   // zero if negative
CMP  wN, #0xFF
CSEL wN, w10, wN, HI       // cap at 0xFF
```

Added new `ARM64_BIC_REG_ASR(d, n, m, shift)` encoding macro.

**9 sites updated:**
- Line 2886: TCA alpha clamp (w4)
- Line 3060: Alpha select ITER_A (w14)
- Line 3094: CCA local select ITER_A (w15)
- Line 3111: CCA local select ITER_Z (w15)
- Line 3280: CCA alpha combine result (w12)
- Line 3318: CCA mux ITER_A local alpha (w4)
- Line 3329: CCA mux ITER_Z local alpha (w4)
- Line 3514: Fog source FOG_ALPHA (w4)
- Line 3524: Fog source FOG_W (w4)

#### File changed:
- `src/include/86box/vid_voodoo_codegen_arm64.h`

---

## Optimization Batch 3: Hoist Params Delta Loads (2026-02-20)

### H1: Hoist loop-invariant RGBA and TMU S/T deltas to prologue

Loaded per-triangle delta vectors into callee-saved NEON registers in the prologue:
- v12 = `{dBdX, dGdX, dRdX, dAdX}` (RGBA color deltas, 4x32-bit)
- v15 = `{dSdX_0, dTdX_0}` (TMU0 texture coordinate deltas, 2x64-bit)
- v14 = `{dSdX_1, dTdX_1}` (TMU1 texture coordinate deltas, conditional on dual TMU)

Replaced per-pixel delta loads in the increment section with direct register usage.
Extended prologue/epilogue STP/LDP to save/restore d14-d15. Stack frame increased
from 160 to 176 bytes.

Net effect: 3 fewer instructions per pixel (4 with dual TMU). No functional change.

#### File modified:
- `src/include/86box/vid_voodoo_codegen_arm64.h`

---

## Optimization Batch 2: Cache STATE_x / STATE_x2 in GPRs (2026-02-20)

### H2: Cache STATE_x in callee-saved w28

Loaded STATE_x once at the top of the pixel loop into w28. Replaced 9 subsequent
per-pixel `LDR w<N>, [x0, #STATE_x]` memory loads with `MOV w<N>, w28`. The cached
value is updated after the end-of-loop x increment store (`MOV w28, w5`).

Sites replaced: stipple test, tiled x calc, depth test address, alpha blend dest
alpha/RGB reads, depth write (both paths), dither/FB write address.

### H3: Cache STATE_x2 in callee-saved w27

Loaded loop bound STATE_x2 once in the prologue into w27. Replaced the per-iteration
`LDR w6, [x0, #STATE_x2]` + `CMP` with `CMP w4, w27`.

Net effect: ~11 fewer memory loads per pixel (10 LDR eliminated, 1 kept for store-back).
No functional change.

#### File modified:
- `src/include/86box/vid_voodoo_codegen_arm64.h`

---

## Optimization Batch 1: Alpha Blend Rounding Cleanup (2026-02-20)

### H7: Replace alookup[1] memory loads with pinned v8 register

Removed 14 `LDR d16, [x20, #16]` instructions that loaded `alookup[1]` = {1,1,1,1} from
memory every time a rounding sequence executed. The identical value is already pinned in
v8 (neon_01_w) since the prologue. Replaced corresponding `ADD v4/v0, v4/v0, v16` with
`ADD v4/v0, v4/v0, v8`.

### H8: Eliminate redundant MOV before USHR in rounding sequences

Removed 14 `MOV v17, v4` (or v0) instructions that copied the source register before
`USHR v17, v17, #8`. USHR supports different Rd/Rn on ARMv8.0, so the copy is unnecessary.
Replaced with `USHR v17, v4, #8` (or `USHR v17, v0, #8`).

Net effect: 28 fewer instructions per alpha-blended pixel (14 LDR + 14 MOV removed).
All 14 sites across dest_afunc (7) and src_afunc (7) switch cases. No functional change.

#### File modified:
- `src/include/86box/vid_voodoo_codegen_arm64.h`

---

## Dead Code Removal (2026-02-20)

### Removed two unused NEON instructions from ARM64 codegen

**MINOR-1**: Removed dead `ARM64_MOV_V(5, 1)` in dual-TMU combine (was line ~2747).
The x86-64 needed to save XMM5 before its multi-instruction multiply sequence
(PMULLW/PMULHW/PUNPCKLWD), but the ARM64 uses `SMULL_4S_4H` which doesn't clobber
its input registers. The saved copy was never read back.

**MINOR-3**: Removed dead `ARM64_MOVI_V2D_ZERO(2)` before depth computation (was line ~2140).
The x86-64 uses XMM2 as a zero register for `PUNPCKLBW` (byte-to-halfword unpack),
but the ARM64 uses `UXTL` instead, which doesn't need a zero operand.

Net effect: 8 fewer bytes of JIT output per generated block. No functional change.

#### File modified:
- `src/include/86box/vid_voodoo_codegen_arm64.h`

---

## Testing Guide Update (2026-02-19)

- Added analyzer script usage to Debug Logging section (`./scripts/analyze-jit-log.py`)
- Added INIT line to log format example
- Updated issue reporting workflow: run analyzer first, attach raw log only if requested

---

## JIT Log Analyzer Script + INIT Logging (2026-02-19)

### Added `scripts/analyze-jit-log.py` — automated JIT health analysis tool

**Motivation:** Manually analyzing 1+ GB JIT debug logs (16M+ lines) is tedious and error-prone.
The Pi 5 user's log required a custom agent run that took ~19 minutes. This script does the same
analysis in a single streaming pass (~2-3 minutes for 16M lines).

#### Script features:
- Single-pass streaming analysis (handles multi-GB logs efficiently)
- **Configuration**: Parses new `VOODOO JIT: INIT` line for render threads, recompiler state, debug level
- **Compilation**: Block count, cache hits, even/odd balance, xdir coverage, recomp range
- **Interpreter fallback detection**: Scans for `INTERPRETER FALLBACK` (use_recompiler=0 or NULL block) and `REJECT` (emit overflow) — patterns the previous manual analysis missed
- **Error scanning**: Checks for crash indicators (SIGILL, SIGSEGV, SIGBUS, fault, trap, etc.)
- **Execution stats**: Scanline counts, total pixels, pixel count distribution
- **Pipeline coverage**: Texture fetch, color combine, alpha test/blend, fog, depth test, dither, FB write — decoded from mode register values
- **Pixel output quality**: Unique RGB565 values, diversity check
- **Iterator health**: Negative value counts (normal for signed Gouraud interpolation)
- **Thread interleave detection**: Counts lines with multiple `VOODOO JIT` prefixes (cosmetic race)
- **Summary table + verdict**: HEALTHY / FUNCTIONAL WITH INTERPRETER FALLBACKS / FUNCTIONAL WITH WARNINGS / COMPILING BUT NOT EXECUTING / JIT NOT ACTIVE

#### Usage:
```bash
./scripts/analyze-jit-log.py <logfile>
```

### Added `VOODOO JIT: INIT` log line (ARM64 only)

Added a single log line at the very start of JIT debug output:
```
VOODOO JIT: INIT render_threads=2 use_recompiler=1 jit_debug=1
```

- Guarded with `#if defined(__aarch64__) || defined(_M_ARM64)` — compiled out on x86-64
- Added to both Voodoo 1/2 and Banshee/3 init paths in `vid_voodoo.c`
- Script falls back to inferring thread count from `odd_even=` values for older logs

#### Files created:
- `scripts/analyze-jit-log.py` — JIT log health analyzer

#### Files modified:
- `src/video/vid_voodoo.c` — Added INIT log line in both init paths (lines ~1177, ~1349)

#### Testing:
- Verified against Pi 5 user log (1.1 GB, 16M lines): matches manual analysis results
- Verified against local macOS log (172 MB, 2.6M lines): INIT line parsed correctly, all stages healthy
- Backwards-compatible with logs missing INIT line (infers thread count from odd_even values)

---

## Readability Review + Comment Pass (2026-02-18)

### Comprehensive comment additions for readability and graphics concept documentation

**Motivation:** Code review found the ARM64 codegen was functionally correct but lacked
high-level explanations of what each pipeline stage does from a 3D graphics perspective,
and several implementation comments were missing or misleading.

**Review document:** `voodoo-arm64-port/code-review-readability.md`

#### Changes applied (36 findings + 5 follow-up fixes):

**Graphics concept block comments (G1-G15):**
- Added file-level overview explaining what a pixel pipeline JIT is and how it works
- Added block comments before every pipeline stage: stipple, tiled framebuffer, W/Z depth,
  depth test, texture fetch, color combine, alpha combine, fog, alpha test, alpha blend,
  dither + FB write, per-pixel increments, JIT cache, and codegen init
- Each comment explains the 3D graphics concept, not just the ARM64 implementation

**High priority fixes (H1-H5):**
- H1: Expanded `tca_reverse_blend_1` divergence comment to clearly explain the upstream bug
- H1: Removed false `0x8E` opcode bug claim at line 1303 (upstream was correct; they also
  independently fixed the `tc_reverse_blend_1` issue after we reported it)
- H2: Added rounding algorithm explanation for the 10+ repeated alpha multiply sequences
- H3: Changed `cc_mselect == 1` to `cc_mselect == CC_MSELECT_CLOCAL` (named constant)
- H4: Documented `dest_aafunc == 4` and `src_aafunc == 4` as `AFUNC_AONE`
- H5: Added variable origin comment at top of `voodoo_generate()` explaining that `tc_*`,
  `cc_*`, `dither*`, `logtable`, `rgb565`, `depth_op` etc. come from the enclosing
  `vid_voodoo_render.c` scope via `#include`

**Medium priority fixes (M1-M9):**
- M1: Added 22-line pipeline stage map at top of `voodoo_generate()`
- M2: Added algorithm overview comment for `voodoo_get_block()` (cache + JIT + fallback)
- M3: Added `voodoo_arm64_data_t` struct purpose comment (valid vs rejected)
- M4: Added bilinear weight geometric explanation (4-corner sub-texel formula)
- M5: Fixed section numbering conflict (two "Section 12" → 12 and 12b)
- M6: Restructured W-depth comment from scratch-pad to reference format
- M7: Explained why depth write is split into two sections (alpha vs Z path)
- M8: Added `loop_jump_pos = block_pos` inline comment marking loop head
- M9: Explained why NEON constants are re-initialized every codegen call

**Low priority fixes (L1-L7):**
- L1: Added semantic purpose for each NEON lookup table (alookup, aminuslookup, etc.)
- L2: Added x16/x17 (IP0/IP1) usage safety note after register table
- L3: Enhanced `STATE_ib` comment to explain the contiguous LD1 4xS32 trick
- L4: Added `1<<48` perspective division rationale (Q12.48 fixed-point format)
- L5: Clarified point-sample LOD+4 (strips 4-bit sub-texel fraction)
- L6: Added `EMIT_MOV_IMM64` scoping note + explanation at later inline repetitions
- L7: Added forward branch patching ordering rationale

**Follow-up fixes from second review pass:**
- Fixed `tc_*/tca_*` inside block comment causing premature `*/` termination (build break)
- Clarified skip patch point comment (skipped pixels commit no data but increments still run)
- Added `IMM7_X` macro comment explaining the divide-by-8 scaled immediate encoding

**Accuracy corrections from third verification pass:**
- Fixed rotating stipple: "rotates each scanline" → "rotates once per pixel" (ROR is inside pixel loop)
- Fixed ADDP description: was claiming lanes [0]+[4], [1]+[5]; corrected to adjacent pairs [0]+[1], [2]+[3]
- Fixed tile dimensions: "128×32 pixels" → "64×32 pixels" (128 is bytes, not pixels at 16bpp)
- Fixed pattern stipple: "bit N corresponds to pixel X & 31" → bit index depends on both X and Y (8×4 pattern)
- Fixed fog blend formula: `(pixel * (255-fog)) + (fogColor * fog)` → actual fixed-point math with denominator 256
- Reworded macro "scoped here" → "#undef'd after use" (C macros aren't scoped by braces)

#### Code change:
- `cc_mselect == 1` → `cc_mselect == CC_MSELECT_CLOCAL` (identical at compile time)

#### Files modified:
- `src/include/86box/vid_voodoo_codegen_arm64.h` — All comment additions

#### Testing:
- Clean build from scratch (cmake configure + full build + codesign)
- Manual regression test: Windows 98 VM with Voodoo card, 3D rendering verified working

---

## Instance State Migration (2026-02-16)

### Moved JIT cache/counter state from file-scope globals to per-instance voodoo_t

**Problem:** The ARM64 JIT codegen used file-scope static variables for cache management
(`last_block[4]`, `next_block_to_write[4]`) and counters (`voodoo_jit_hit_count`,
`voodoo_jit_gen_count`, `voodoo_recomp`). With up to 4 render threads per Voodoo instance,
these globals create race conditions and incorrect behavior with multiple VM instances.

**Solution:** Added 7 new fields to `voodoo_t` in `vid_voodoo_common.h`:
- `jit_last_block[4]` -- per-thread cache LRU hint (was `last_block[4]`)
- `jit_next_block_to_write[4]` -- per-thread round-robin write index (was `next_block_to_write[4]`)
- `jit_recomp` -- recompilation counter (was `voodoo_recomp`)
- `jit_hit_count` -- cache hit counter (was `voodoo_jit_hit_count`)
- `jit_gen_count` -- code generation counter (was `voodoo_jit_gen_count`)
- `jit_exec_count` -- execution counter (was `voodoo_jit_exec_count` in render.c)
- `jit_verify_mismatches` -- verify-mode mismatch counter (was static local in render.c)

**Files modified:**
- `src/include/86box/vid_voodoo_common.h` -- Added 7 fields after `codegen_data`
- `src/include/86box/vid_voodoo_codegen_arm64.h` -- Removed global statics, updated
  `voodoo_get_block()` and `voodoo_codegen_init()` to use instance state
- `src/video/vid_voodoo_render.c` -- Updated JIT exec/verify counters to use instance state

**Notes:**
- `int voodoo_recomp = 0` global kept to satisfy `extern` in `vid_voodoo_render.h` (used by
  x86-64 codegen); ARM64 path increments both global and per-instance
- NEON lookup tables remain as file-scope statics (read-only after init, shared safely)
- x86-64 codegen left unchanged (separate compilation path)

**Verification:**
- ✅ Clean build succeeds (macOS ARM64, all 911 objects link cleanly)
- ✅ VM launches without crash (Windows 98 test VM with Voodoo3, render_threads=4)
- ✅ No race conditions observed (per-instance cache state prevents concurrent access issues)

---

## ARM64 Codegen Follow-up Fixes (2026-02-16)

### Targeted correctness + safety fixes in `voodoo_get_block()` / emit path

**1) Cache probe order bug fixed**
- **Problem:** `voodoo_get_block()` seeded probe start from `jit_last_block[odd_even]`, but lookup indexed by loop counter `c`, so the cache hint was ignored.
- **Change:** Probe now starts at the hinted block and wraps with `(start + c) & 7`.
- **Why:** Restores intended locality behavior for cache hits without changing cache key semantics.

**2) NEON save/restore comment corrected**
- **Problem:** Register assignment comment claimed `v12-v15` save/restore, but prologue/epilogue only save/restore `d12-d13` (plus constant regs `d8-d11`).
- **Change:** Comment now states `v12-v13` are the scratch callee-saved SIMD regs actually preserved by generated code; `v14-v15` are not used.
- **Why:** Keeps documentation aligned with the real ABI behavior and avoids misleading maintenance assumptions.

**3) JIT emit overflow now falls back to interpreter (no VM abort)**
- **Problem:** `arm64_codegen_check_emit_bounds()` called `fatal()` on oversized blocks, terminating the process.
- **Change:** Emission overflow is now tracked per-thread during codegen; when overflow is detected:
  - emission stops before out-of-bounds write,
  - generated slot is marked invalid,
  - W^X is restored,
  - `voodoo_get_block()` returns `NULL` so render path uses interpreter fallback.
- **Why:** Preserves strict bounds safety while preventing hard process aborts on rare oversized codegen cases.

**4) Counter increment race risk reduced**
- **Problem:** `jit_hit_count`, `jit_gen_count`, `jit_recomp` were incremented non-atomically in `voodoo_get_block()`.
- **Change:** Converted these per-instance JIT counters to `ATOMIC_INT` and switched increments/stores in ARM64 codegen path to project atomic primitives (`ATOMIC_INC`, `ATOMIC_STORE`, `ATOMIC_LOAD`).
- **Why:** Keeps debug/recomp stats semantics close to existing behavior while reducing multithread update races.
- **Follow-up:** Removed ARM64-side increment of legacy global `voodoo_recomp`; ARM64 now uses only per-instance `voodoo->jit_recomp`.

**5) Cache slot validity tracking added**
- **Problem:** A partially-emitted overflow block could leave stale metadata risk in the selected cache slot.
- **Change:** Added per-slot `valid` bit in ARM64 codegen cache data; lookup requires `valid`, success path sets it, overflow path clears it.
- **Why:** Prevents accidental reuse of invalid/partial code blocks after rejected generation.

**6) MAP_JIT init crash fix (macOS)**
- **Problem:** Zeroing the whole executable `codegen_data` mapping during `voodoo_codegen_init()` can fault if thread JIT write-protect is already enabled.
- **Change:** Removed eager `memset` of the executable mapping in init; validity still starts cleared because anonymous mmap pages are zero-initialized.
- **Why:** Prevents startup SIGBUS/KERN_PROTECTION_FAILURE in `voodoo_codegen_init` while preserving cache-slot validity semantics.

**7) Overflow-key memoization to avoid repeated regen churn**
- **Problem:** Repeatedly hitting the same oversized JIT key caused repeated generation attempts before interpreter fallback.
- **Change:** Added `rejected` state in ARM64 cache entries; overflow now stores the cache key and marks slot rejected so future identical probes fast-return interpreter fallback without regenerating.
- **Why:** Reduces avoidable JIT overhead/stutter on persistent overflow keys while keeping bounds safety strict.

---

## Phase 1: Scaffolding + Prologue/Epilogue

### 2026-02-15 -- Phase 1 started

- Created `voodoo-arm64-port/CHANGELOG.md` to track all changes
- Branch: `phase-1-scaffolding`

#### Files created:
- `src/include/86box/vid_voodoo_codegen_arm64.h` -- ARM64 JIT codegen header

#### Files modified:
- `src/include/86box/vid_voodoo_render.h` -- Added ARM64 to NO_CODEGEN gate
- `src/video/vid_voodoo_render.c` -- Added ARM64 include path

#### Key decisions:
- Register allocation: x0=state, x1=params, x2=x, x3=real_y, x19-x28=callee-saved pinned values
- NEON constants in v8-v11 (callee-saved): xmm_01_w, xmm_ff_w, xmm_ff_b, minus_254
- W^X toggle via `pthread_jit_write_protect_np()` in `voodoo_get_block()`
- I-cache flush via `__clear_cache()` (cross-platform, matches codegen_allocator.c)
- NEON lookup tables use `uint16_t[8]` arrays (no SIMD intrinsics in header)
- Upstream bug at x86-64 line 1303 (0x8E instead of 0x83) will NOT be ported

#### Build fixes:
- Added `#include <stdint.h>` to resolve uint8_t/uint16_t/uint32_t types
- Build verified on macOS ARM64: all 50 static offset assertions pass, links cleanly

#### Testing:
- **Runtime test passed** (2026-02-15): Emulator launches, Voodoo card initializes, rendering falls through to interpreter as expected (pixel pipeline not yet implemented)

---

## Phase 2: Pixel Loop + Depth

### 2026-02-15 -- Phase 2 complete

- Branch: `phase-2-pixel-loop-depth`

#### Implementation:
- **Pixel loop structure**: X-coordinate iteration with xdir (forward/backward) support
- **Stipple test**: Pattern stipple (bit lookup from real_y/x) and rotating stipple (ROR + TBZ)
- **Tiled framebuffer**: X-coordinate calculation for tiled modes
- **W-depth computation**: CLZ-based (ARM64 BSR equivalent) with clamping to 0..0xFFFF
- **Z-buffer depth**: SAR 12 with signed clamping, depth bias (zaColor addition)
- **All 8 DEPTHOP modes**: NEVER (immediate RET), LESSTHAN, EQUAL, LESSTHANEQUAL, GREATERTHAN, NOTEQUAL, GREATERTHANEQUAL, ALWAYS
- **Per-pixel state increments**: RGBA via NEON 4xS32, Z via GPR, TMU S/T via NEON 2xD64, TMU W and global W via GPR 64-bit
- **Forward branch patching**: Macros for BCOND, B, TBxZ, CBxZ
- **Bitmask immediates**: AND/ANDS/ORR/TST with ARM64 bitmask encoding

#### Key decisions:
- Depth test uses unsigned comparison (values 0..0xFFFF) with ARM64 condition codes: CS (>=), HI (>), LS (<=), CC (<)
- Texture fetch calls are placeholders for Phase 3 integration

#### Testing:
- **Runtime test passed** (2026-02-15): Quake 3 renders black screen, 3DMark 99 renders gray screen (expected - color/texture pipeline not yet implemented). No crashes, depth pipeline executes correctly.

---

## Phase 3: Texture Fetch

### 2026-02-15 -- Phase 3 complete

- Branch: `phase-3-texture-fetch`
- PR: #3

#### Implementation:
- **`codegen_texture_fetch()`**: Complete texture sampling pipeline
- **Perspective-correct W division**: SDIV instruction for 1/W calculation
- **LOD calculation**: CLZ-based (ARM64 BSR equivalent) with clamping
- **Point-sampled lookup**: Direct texel fetch from texture RAM
- **Bilinear filtering**: NEON 4-tap weighted blend (horizontal + vertical)
- **Texture coordinate wrapping**: Mirror S/T and Clamp S/T modes
- **TMU configuration**: TMU0-only, TMU1-only (passthrough), and dual-TMU combine
- **Dual-TMU combine modes**: tc_mselect (ZERO, C_LOCAL, A_OTHER, A_LOCAL, TEX, TEXRGB), tc_add, tc_invert
- **trexInit1 override**: Texture parameter override path

#### Bug fixes:
- **Upstream bug NOT ported**: x86-64 line 1303 emits invalid opcode (0x8E MOV ES,AX instead of 0x83 ADD EAX,1). ARM64 implementation uses correct ADD encoding.
- **Second upstream quirk**: x86-64 line 3053 uses ROR instead of SHR (functionally equivalent in context, both implementations valid)

#### Key decisions:
- Texture lookups use NEON vector operations for RGBA unpacking and blending
- LOD calculation matches x86-64 bit-manipulation semantics exactly
- Dual-TMU combine uses the same logical pipeline as single-TMU for consistency

---

## Phase 4: Color/Alpha Combine

### 2026-02-15 -- Phase 4 complete

- Branch: `phase-4-color-alpha`

#### Implementation:
- **Color select (CC_LOCALSELECT)**: ITER_RGB (Gouraud shading), TEX (texture color), COLOR1 (constant), LFB (framebuffer read)
- **Local color override**: tex_a bit 7 control path
- **Chroma key test**: RGB comparison against chromaKey register with reject path
- **Alpha select (A_SEL)**: ITER_A (Gouraud alpha), TEX (texture alpha), COLOR1 (constant)
- **CCA local select**: ITER_A, COLOR0, ITER_Z modes
- **Alpha mask test**: Alpha value comparison against alphaMask register
- **Color combine pipeline**:
  - cc_zero_other: Zero "other" color input
  - cc_sub_clocal: Subtract "local" from "other"
  - cc_mselect: Multiply select (ZERO, CLOCAL, AOTHER, ALOCAL, TEX, TEXRGB)
  - cc_reverse_blend: Reverse blend direction
  - cc_add: Add mode (CLOCAL or ALOCAL)
  - cc_invert_output: Invert final RGB output
- **Alpha combine pipeline**: cca_zero_other, cca_sub_clocal, cca_mselect, cca_reverse_blend, cca_add, cca_invert_output (parallel to color combine)
- **Output clamping**: All RGBA channels clamped to [0, 255]

#### Key decisions:
- Color and alpha pipelines implemented as parallel paths following x86-64 structure
- NEON vector operations used for parallel RGB channel processing
- All combine modes match x86-64 bit-exact behavior
- Clamping uses NEON SMIN/SMAX for efficient saturation

#### Testing:
- **Build verified** (2026-02-15): Compiles cleanly, all static assertions pass
- **JIT validation logging added**: Logs codegen events for debugging
- **Validation complete** (2026-02-15): All 4 phases validated against 3dfx specs by voodoo-arch agents

#### Post-Phase 4 Fixes (2026-02-15)

**Frame size optimization**: Reduced stack frame from 144 to 128 bytes (-16 bytes per call)
- Validation found prologue comment claimed d14/d15 saved at SP-144, but actual code only saves d8-d13
- Since v14/v15 are never used, removed the misleading comment and reduced frame size
- Stack remains 16-byte aligned (128 = 8 × 16)

---

## Phase 5: Fog + Alpha Test + Alpha Blend

### 2026-02-16 -- Phase 5 complete (merged with Phase 6)

- Branch: `phase-5-6-effects`
- Commit: 57e5c6fe1

#### Implementation:
- **Fog pipeline (`codegen_fog()`)**: Complete APPLY_FOG macro equivalent
  - FOG_CONSTANT mode: Direct fogColor addition
  - General fog: FOG_ADD, FOG_MULT, fog delta modes
  - Fog alpha sources: W-depth (fog table lookup with delta interpolation), Z, iterated alpha, W
  - Fog table lookup: `fogTable[idx].fog + (fogTable[idx].dfog * frac) >> 10`
  - Fog blending: `(fog_rgb * fog_a) >> 8` with add/multiply modes
  - Final output clamping to [0, 255]
- **Alpha test (`codegen_alpha_test()`)**: Complete ALPHA_TEST macro equivalent
  - All 8 AFUNC modes: NEVER, LESSTHAN, EQUAL, LESSTHANEQUAL, GREATERTHAN, NOTEQUAL, GREATERTHANEQUAL, ALWAYS
  - Uses conditional branch to skip_pixel label on failure
- **Alpha blend (`codegen_alpha_blend()`)**: Complete ALPHA_BLEND macro equivalent
  - Destination alpha functions (dest_afunc): AZERO, ASRC_ALPHA, A_COLOR, ADST_ALPHA, AONE, AOMSRC_ALPHA, AOM_COLOR, AOMDST_ALPHA, ACOLORBEFOREFOG
  - Source alpha functions (src_afunc): AZERO, ASRC_ALPHA, A_COLOR, ADST_ALPHA, AONE, AOMSRC_ALPHA, AOM_COLOR, AOMDST_ALPHA, ASATURATE
  - Division by 255 optimized using `alookup[]` NEON table (multiply + shift)
  - Result: `src_rgb = (src_rgb * src_factor + dest_rgb * dest_factor)` clamped to [0,255]

---

## Phase 6: Dither + Framebuffer Write + Depth Write + Per-Pixel Increments

### 2026-02-16 -- Phase 6 complete (merged with Phase 5)

#### Implementation:
- **Dithering (`codegen_dither()`)**: Complete dither support
  - 4x4 dither: 16-entry lookup table indexed by `(value*16) + (y & 3)*4 + (x & 3)`
  - 2x2 dither: 4-entry lookup table indexed by `(value*4) + (y & 1)*2 + (x & 1)`
  - No-dither path: Direct 8-bit to 5/6-bit truncation
  - Per-channel dither table lookup via LDRB
- **RGB565 packing**: R(5):G(6):B(5) bitfield assembly with ORR/LSL
- **Framebuffer write**: STRH to `fb_mem + x*2` (16-bit color write)
- **Alpha write**: STRH to `aux_mem + x*2` (16-bit alpha+depth aux buffer)
- **Depth write**: STRH to `aux_mem + x*2` (conditional on depth_op != DEPTHOP_NEVER)
- **Per-pixel color increment**: NEON LD1/ADD/ST1 for {ib, ig, ir, ia} += {dBdX, dGdX, dRdX, dAdX}
- **Per-pixel Z increment**: GPR LDR/ADD/STR for z += dZdX
- **Per-pixel TMU S/T increments**: NEON LD1/ADD/ST1 for {tmu_s, tmu_t} += {dSdX, dTdX} (both TMU0 and TMU1)
- **Per-pixel TMU W increment**: GPR LDR/ADD/STR for tmu_w += dWdX (both TMU0 and TMU1)
- **Per-pixel global W increment**: GPR LDR/ADD/STR for w += dWdX
- **X increment and loop**: x += xdir, compare against x2, branch back to loop top

---

## Post-Phase 5+6 Debugging: Horizontal Striping Corruption

### 2026-02-16 -- Critical encoding bug found and fixed

**Symptom**: Horizontal striping and color distortion in rendered output. Consistent within scanlines, varying between scanlines. Corruption persisted across all tested configurations.

#### Bugs fixed during initial debugging (no visual change):

1. **Dither table pointer load** -- Was using wrong register sequence for table base address
2. **Alpha blend shift amount** -- `LSL_IMM(6, 5, 6)` should be `LSL_IMM(6, 5, 7)` for dest/src alpha blend factor computation (both dest_aafunc==4 and src_aafunc==4 paths)
3. **Fog alookup offset** -- `LDR_D(5, 5, 0)` should be `LDR_D(5, 5, 16)` to match x86-64's `+16` byte offset into alookup table
4. **Stack frame size** -- Increased from 128 to 160 bytes to accommodate additional callee-saved registers
5. **REV16 macro added** -- Added missing `ARM64_REV16(d, n)` encoding macro

#### ROOT CAUSE: LD1/ST1 opcode encoding (2-register instead of 1-register)

**The bug**: `ARM64_LD1_V4S` and `ARM64_ST1_V4S` macros used opcode field `1010` (2-register form) instead of `0111` (1-register form):

```c
// WRONG: bits [15:12] = 1010 = 2 registers -- loads/stores Vt AND V(t+1)
#define ARM64_LD1_V4S(t, n) (0x4C40A800 | Rn(n) | Rt(t))
#define ARM64_ST1_V4S(t, n) (0x4C00A800 | Rn(n) | Rt(t))

// CORRECT: bits [15:12] = 0111 = 1 register -- loads/stores only Vt
#define ARM64_LD1_V4S(t, n) (0x4C407800 | Rn(n) | Rt(t))
#define ARM64_ST1_V4S(t, n) (0x4C007800 | Rn(n) | Rt(t))
```

**Impact**: Every LD1/ST1 loaded/stored 32 bytes (2 NEON registers) instead of 16 bytes:
- **ST1 at per-pixel increment (line 3782)**: Stored v0 AND v1 to `&state->ib`, corrupting `state->z`, `state->new_depth`, and `state->tmu0_s` with delta values
- **ST1 at TMU1 increment (line 3851)**: Stored v0 AND v1 to `&state->tmu1_s`, corrupting `state->tmu1_w` and `state->w`
- **LD1 at color combine (line 2844)**: Loaded v0 AND v1, clobbering previously computed clocal value in v1

**Why it caused horizontal striping**: Each pixel corrupted the next pixel's texture coordinates and depth. The error accumulated across the span (left to right), but each scanline was reset by the C caller, creating consistent per-scanline patterns that appeared as horizontal banding.

**Why three audits missed it**: All audits verified logic (register allocation, data flow, branch targets) but not the raw instruction encoding constants. The 2-bit difference between `0xA8` and `0x78` in the opcode field is visually subtle.

**Fix**: Two bytes changed in two macro definitions (lines 859, 862).

**Independently verified** by GPT-5 cross-referencing ARM64 ISA manual and disassembly.

#### Full analysis: `voodoo-arm64-port/opus-deep-analysis-2026-02-16.md`

---

## JIT Debug Logging Toggle

### 2026-02-16 -- Runtime debug toggle added

Replaced compile-time `#define VOODOO_JIT_DEBUG 1` / `VOODOO_JIT_DEBUG_EXEC 1` with a runtime UI toggle.

#### Files modified:
- `src/include/86box/vid_voodoo_common.h` -- Added `jit_debug` and `jit_debug_log` fields to `voodoo_t`
- `src/video/vid_voodoo.c` -- Added CONFIG_BINARY entry to `voodoo_config[]`, init (fopen) in both init paths, close (fclose) in `voodoo_card_close()`
- `src/video/vid_voodoo_banshee.c` -- Added CONFIG_BINARY entry to all 3 Banshee config arrays (`banshee_sgram_config`, `banshee_sgram_16mbonly_config`, `banshee_sdram_config`)
- `src/include/86box/vid_voodoo_codegen_arm64.h` -- Removed `#define VOODOO_JIT_DEBUG`, replaced `#if`/`pclog()` with `if (voodoo->jit_debug)`/`fprintf()`
- `src/video/vid_voodoo_render.c` -- Removed `#define VOODOO_JIT_DEBUG_EXEC`, replaced `#if`/`pclog()` with `if (voodoo->jit_debug)`/`fprintf()`

#### Files created:
- `voodoo-arm64-port/jit-debug-removal.md` -- Removal guide for when debug logging is no longer needed

#### Key decisions:
- Toggle is inside `#ifndef NO_CODEGEN` so it only appears when JIT is available
- Log file: `<vm_directory>/voodoo_jit.log` (created fresh each session when ON)
- `jit_debug` flag is purely observational -- never affects JIT-vs-interpreter control flow
- Default OFF (no performance impact when disabled)

---

## Voodoo 2 Detection Fix

### 2026-02-16 -- Non-perspective texture path alignment bug

**Symptom**: Voodoo 2 card not detected by Windows 98 when ARM64 JIT "Dynamic Recompiler" is ON. Works with interpreter (recompiler OFF). Voodoo 3 unaffected.

**Root cause**: ARM64 unsigned-offset encoding silently truncates unaligned offsets. `STR_X` (64-bit store) to `STATE_tex_s` (offset 188) encoded as offset 184 because `OFFSET12_X(188) = (188 >> 3) << 10 = 23 << 10`, and `23 * 8 = 184`, not 188.

On x86-64, `MOV [mem], RAX` has no alignment restriction — stores 8 bytes to any address. On ARM64, the unsigned-offset form of `STR Xt` scales by 8 and silently drops the remainder.

**Impact**: In the non-perspective texture path (textureMode bit 0 clear), `tex_s` was written to offset 184 instead of 188. When the point-sampled or bilinear code read `tex_s` from offset 188 (via `LDR_W`, which is 4-byte aligned and correct), it read the upper 32 bits of the 64-bit value — always zero. Every pixel sampled from texture column S=0, producing identical stale texel values.

**Why Voodoo 2 specifically**: Voodoo 2 is a 3D-only pass-through card. The Windows driver probe renders test patterns to the framebuffer and reads them back to verify the card works. The corrupted texture output caused the probe to fail, preventing detection. Voodoo 3 (integrated 2D+3D) uses a different detection path.

**Discovery method**: JIT verification mode (jit_debug=2) runs both JIT and interpreter per scanline, comparing pixel output. Caught mismatches where pixel[0] was correct but pixels 1+ all had the same stale value (0x6eb0).

**Fix**: Changed `STR_X` to `STR_W` for `STATE_tex_s` and `STATE_tex_t` stores in the non-perspective texture path. The consuming reads are all 32-bit (`LDR_W`), and `STR_W` only requires 4-byte alignment (188/4 = 47 exact).

**Alignment audit**: All other `STR_X`/`LDR_X` (8-byte) and `STR_Q`/`LDR_Q` (16-byte) offsets in the file verified as properly aligned. Non-aligned 128-bit accesses (STATE_ib at 472, STATE_tmu1_s at 520) already use `ADD_IMM_X` + `LD1/ST1` which have no alignment requirement.

#### File modified:
- `src/include/86box/vid_voodoo_codegen_arm64.h` -- Lines 1189, 1195: `STR_X` → `STR_W`

---

## Comment Cleanup

### 2026-02-16 -- Three rounds of comment audits

Audited all ~1450 comment lines in the ARM64 codegen header for accuracy.

#### Round 1 -- 12 issues fixed:
- Fixed factually wrong comments: `z >> 20` → `z >> 12`, stack frame 128 → 160 bytes, `alookup[fog_a]` → `alookup[fog_a + 1]`
- Replaced x86 mnemonics used as primary descriptions: CMOVS/CMOVAE → CSEL, XOR → EOR, SHR → LSR, SAR → ASR
- Consolidated contradictory W-depth comments, cleaned up rambling alookup comment
- Removed false "v2 = zero from earlier" claim, replaced vestigial x86-64 register block with pointer
- Fixed stale "save d12-d15" → "save d12-d13"

#### Round 2 -- 9 issues fixed:
- Replaced remaining x86 mnemonics: SHL → LSL, IMUL → MUL, PADDW → ADD v4.4H
- Fixed x86 register names: EBX/ebx → w5, xmm_00_ff_w → neon_00_ff_w
- Corrected alpha blend shift comments to reflect doubled input values

#### Round 3 -- 2 final nits:
- Line reference `~51` → `~45` for register assignment block
- Stale "Phase 1: Prologue and epilogue only" → "complete pixel pipeline (Phases 1-6)"

---

## Hardening: Emission Safety

### 2026-02-16 -- ARM64 codegen guardrails

#### Implementation:
- Added explicit `#include <string.h>` in ARM64 codegen header (for `memset`)
- Added emission bounds guard: `arm64_codegen_check_emit_bounds()` and wired `addlong(...)` through it
- Added forward-branch patch guards:
  - `arm64_codegen_check_patch_pos()`
  - `arm64_codegen_check_branch_offset()` for `imm14`/`imm19`/`imm26` range + 4-byte alignment
  - Applied to `PATCH_FORWARD_BCOND`, `PATCH_FORWARD_B`, `PATCH_FORWARD_TBxZ`, `PATCH_FORWARD_CBxZ`

#### Key decisions:
- Guard checks are always active (not debug-only) to fail fast on silent JIT corruption paths
- Changes are ARM64-local and do not modify x86/x86-64 backends

#### File modified:
- `src/include/86box/vid_voodoo_codegen_arm64.h`

### 2026-02-16 -- 4-thread crash regression investigation

**Symptom**: VM crashed immediately with `render_threads=4` during the state-locality experiment.

**Outcome**: The state-locality refactor was rolled back pending a safer design. Current code keeps the previous cache bookkeeping model and retains only emission/patch guard hardening.

---

## Summary

| Phase | Description | Status | PR |
|-------|-------------|--------|----|
| 1 | Scaffolding + Prologue/Epilogue | Merged | #1 |
| 2 | Pixel Loop + Depth Test + Stipple | Merged | #2 |
| 3 | Texture Fetch + LOD + Bilinear + TMU Combine | Merged | #3 |
| 4 | Color/Alpha Combine + Chroma Key | Merged | #4 |
| 5 | Fog + Alpha Test + Alpha Blend | Committed | -- |
| 6 | Dither + FB Write + Depth Write + Increments | Committed | -- |
| Debug | LD1/ST1 encoding fix + 5 other fixes | Committed | -- |
| Infra | JIT debug logging runtime toggle | Committed | -- |
| Fix | Voodoo 2 non-perspective texture alignment bug | Committed | -- |
| Hardening | Emission bounds checks + branch patch validation | Committed | -- |
| Tool | JIT log analyzer script + INIT log line | Committed | -- |
| Opt 1-7 | Round 1 optimization batches (80-100 insns/pixel removed) | Committed | -- |
| Opt 7-fix | Batch 7/M5 TMU0 alpha extraction ordering bugfix | Committed | -- |
| Opt 8 | Round 2 Batch 8: loop + cc + stipple peepholes (~4 insn/px) | Testing | -- |
